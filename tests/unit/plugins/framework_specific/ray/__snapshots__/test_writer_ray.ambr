# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_complex_h]
  "RuntimeError('Ray workflows do not currently support multiple artifacts being returned as sink nodes.\\n                Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_hidden_session_dependencies]
  '
  def get_a():
      b0 = 0
      a = b0 + 1
      return a
  
  
  def get_linear_first_for_artifact_linear_second_and_downstream():
      linear_first = 1
      return linear_first
  
  
  def get_linear_second(linear_first):
      linear_second = linear_first + 1
      return linear_second
  
  
  def get_linear_third(linear_first, linear_second):
      linear_third = linear_second + linear_first
      return linear_third
  
  
  def run_session_including_a():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a = get_a()
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_session_including_linear_second():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      linear_first = get_linear_first_for_artifact_linear_second_and_downstream()
      linear_second = get_linear_second(linear_first)
      artifacts["linear_second"] = copy.deepcopy(linear_second)
      linear_third = get_linear_third(linear_first, linear_second)
      artifacts["linear_third"] = copy.deepcopy(linear_third)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a())
      artifacts.update(run_session_including_linear_second())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_hidden_session_dependencies].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_hidden_session_dependencies].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_hidden_session_dependencies_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_a():
  
      a = ray_hidden_session_dependencies_module.get_a()
  
      return a
  
  
  @ray.remote
  def task_linear_first_for_artifact_linear_second_and_downstream():
  
      linear_first = (
          ray_hidden_session_dependencies_module.get_linear_first_for_artifact_linear_second_and_downstream()
      )
  
      return linear_first
  
  
  @ray.remote
  def task_linear_second(linear_first):
  
      linear_second = ray_hidden_session_dependencies_module.get_linear_second(
          linear_first
      )
  
      return linear_second
  
  
  @ray.remote
  def task_linear_third(linear_first, linear_second):
  
      linear_third = ray_hidden_session_dependencies_module.get_linear_third(
          linear_first, linear_second
      )
  
      return linear_third
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  a = task_a.bind()
  linear_first = task_linear_first_for_artifact_linear_second_and_downstream.bind()
  linear_second = task_linear_second.bind(linear_first)
  linear_third = task_linear_third.bind(linear_first, linear_second)
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(linear_third)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_a0_b0]
  "RuntimeError('Ray workflows do not currently support multiple artifacts being returned as sink nodes.\\n                Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_a0_b0_dependencies]
  "RuntimeError('Ray workflows do not currently support multiple artifacts being returned as sink nodes.\\n                Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_a_b0_inputpar]
  '
  import argparse
  
  
  def get_b0(b0):
  
      return b0
  
  
  def get_a(b0):
      a = b0 + 1
      return a
  
  
  def run_session_including_b0(b0=0):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0(b0)
      artifacts["b0"] = copy.deepcopy(b0)
      a = get_a(b0)
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_all_sessions(
      b0=0,
  ):
      artifacts = dict()
      artifacts.update(run_session_including_b0(b0))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--b0", type=int, default=0)
      args = parser.parse_args()
      artifacts = run_all_sessions(
          b0=args.b0,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_a_b0_inputpar].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_a_b0_inputpar].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_a_b0_inputpar_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_b0(b0):
  
      b0 = int(b0)
  
      b0 = ray_pipeline_a_b0_inputpar_module.get_b0(b0)
  
      return b0
  
  
  @ray.remote
  def task_a(b0):
  
      a = ray_pipeline_a_b0_inputpar_module.get_a(b0)
  
      return a
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {"b0": 0}
  
  b0 = task_b0.bind(pipeline_arguments["b0"])
  a = task_a.bind(b0)
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(a)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_multiple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_multiple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  packaging==21.3
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_multiple].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_housing_multiple_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_assets_for_artifact_y_and_downstream():
  
      assets = (
          ray_pipeline_housing_multiple_module.get_assets_for_artifact_y_and_downstream()
      )
  
      return assets
  
  
  @ray.remote
  def task_y(assets):
  
      y = ray_pipeline_housing_multiple_module.get_y(assets)
  
      return y
  
  
  @ray.remote
  def task_p_value(assets, y):
  
      p = ray_pipeline_housing_multiple_module.get_p_value(assets, y)
  
      return p
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  assets = task_assets_for_artifact_y_and_downstream.bind()
  y = task_y.bind(assets)
  p = task_p_value.bind(assets, y)
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(p)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_simple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_p_value():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      clf = RandomForestClassifier(random_state=0)
      y = assets["is_new"]
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_p_value():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      p = get_p_value()
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_p_value())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_simple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  packaging==21.3
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_simple].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_housing_simple_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_p_value():
  
      p = ray_pipeline_housing_simple_module.get_p_value()
  
      return p
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  p = task_p_value.bind()
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(p)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_w_dependencies]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_w_dependencies].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  packaging==21.3
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_housing_w_dependencies].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_housing_w_dependencies_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_assets_for_artifact_y_and_downstream():
  
      assets = (
          ray_pipeline_housing_w_dependencies_module.get_assets_for_artifact_y_and_downstream()
      )
  
      return assets
  
  
  @ray.remote
  def task_y(assets):
  
      y = ray_pipeline_housing_w_dependencies_module.get_y(assets)
  
      return y
  
  
  @ray.remote
  def task_p_value(assets, y):
  
      p = ray_pipeline_housing_w_dependencies_module.get_p_value(assets, y)
  
      return p
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  assets = task_assets_for_artifact_y_and_downstream.bind()
  y = task_y.bind(assets)
  p = task_p_value.bind(assets, y)
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(p)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_two_input_parameter]
  '
  import argparse
  
  
  def get_pn(n, p):
      pn = p * n
      return pn
  
  
  def run_session_including_pn(
      p="p",
      n=5,
  ):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      pn = get_pn(n, p)
      artifacts["pn"] = copy.deepcopy(pn)
      return artifacts
  
  
  def run_all_sessions(
      n=5,
      p="p",
  ):
      artifacts = dict()
      artifacts.update(run_session_including_pn(p, n))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--n", type=int, default=5)
      parser.add_argument("--p", type=str, default="p")
      args = parser.parse_args()
      artifacts = run_all_sessions(
          n=args.n,
          p=args.p,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_two_input_parameter].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_task_per_artifact-ray_pipeline_two_input_parameter].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_two_input_parameter_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_pn(n, p):
  
      n = int(n)
  
      p = str(p)
  
      pn = ray_pipeline_two_input_parameter_module.get_pn(n, p)
  
      return pn
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {"p": "p", "n": 5}
  
  pn = task_pn.bind(pipeline_arguments["n"], pipeline_arguments["p"])
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(pn)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_complex_h]
  "RuntimeError('Ray workflows do not currently support tasks with multiple returns. Task run_session_including_f has 2 returns.\\n                    Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_hidden_session_dependencies]
  "RuntimeError('Ray workflows do not currently support tasks with multiple returns. Task run_session_including_linear_second has 2 returns.\\n                    Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_a0_b0]
  "RuntimeError('Ray workflows do not currently support multiple artifacts being returned as sink nodes.\\n                Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_a0_b0_dependencies]
  '
  def get_b0():
      b0 = 0
      return b0
  
  
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_b0())
      artifacts.update(run_session_including_a0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_a0_b0_dependencies].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_a0_b0_dependencies].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_a0_b0_dependencies_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_run_session_including_b0():
  
      artifacts = ray_pipeline_a0_b0_dependencies_module.run_session_including_b0()
  
      b0 = artifacts["b0"]
  
      return b0
  
  
  @ray.remote
  def task_run_session_including_a0():
  
      artifacts = ray_pipeline_a0_b0_dependencies_module.run_session_including_a0()
  
      a0 = artifacts["a0"]
  
      return a0
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  b0 = task_run_session_including_b0.bind()
  a0 = task_run_session_including_a0.bind()
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(a0)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_a_b0_inputpar]
  "RuntimeError('Ray workflows do not currently support tasks with multiple returns. Task run_session_including_b0 has 2 returns.\\n                    Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_housing_multiple]
  "RuntimeError('Ray workflows do not currently support tasks with multiple returns. Task run_session_including_y has 2 returns.\\n                    Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_housing_simple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_p_value():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      clf = RandomForestClassifier(random_state=0)
      y = assets["is_new"]
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_p_value():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      p = get_p_value()
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_p_value())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_housing_simple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  packaging==21.3
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_housing_simple].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_housing_simple_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_run_session_including_p_value():
  
      artifacts = ray_pipeline_housing_simple_module.run_session_including_p_value()
  
      p_value = artifacts["p value"]
  
      return p_value
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  p_value = task_run_session_including_p_value.bind()
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(p_value)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_housing_w_dependencies]
  "RuntimeError('Ray workflows do not currently support tasks with multiple returns. Task run_session_including_y has 2 returns.\\n                    Consider use use_workflows=False to disable using Ray Workflows API.')"
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_two_input_parameter]
  '
  import argparse
  
  
  def get_pn(n, p):
      pn = p * n
      return pn
  
  
  def run_session_including_pn(
      p="p",
      n=5,
  ):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      pn = get_pn(n, p)
      artifacts["pn"] = copy.deepcopy(pn)
      return artifacts
  
  
  def run_all_sessions(
      n=5,
      p="p",
  ):
      artifacts = dict()
      artifacts.update(run_session_including_pn(p, n))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--n", type=int, default=5)
      parser.add_argument("--p", type=str, default="p")
      args = parser.parse_args()
      artifacts = run_all_sessions(
          n=args.n,
          p=args.p,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_two_input_parameter].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_task_per_session-ray_pipeline_two_input_parameter].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_two_input_parameter_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_run_session_including_pn(p, n):
  
      p = str(p)
  
      n = int(n)
  
      artifacts = ray_pipeline_two_input_parameter_module.run_session_including_pn(p, n)
  
      pn = artifacts["pn"]
  
      return pn
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {"p": "p", "n": 5}
  
  pn = task_run_session_including_pn.bind(
      pipeline_arguments["p"], pipeline_arguments["n"]
  )
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(pn)
  
  '
---
