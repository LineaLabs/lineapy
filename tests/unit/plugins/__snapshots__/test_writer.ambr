# name: test_pipeline_generation[airflow_complex_h_perartifact]
  '
  def get_a_c_for_artifact_f_and_downstream():
      a0 = 0
      a0 += 1
      a = 1
      a += 1
      b = a * 2 + a0
      c = b + 3
      return a, c
  
  
  def get_f(c):
      f = c + 7
      return f
  
  
  def get_h(a, c):
      d = a * 4
      e = d + 5
      e += 6
      a += 1
      g = c + e * 2
      h = a + g
      return h
  
  
  def run_session_including_f():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a, c = get_a_c_for_artifact_f_and_downstream()
      f = get_f(c)
      artifacts["f"] = copy.deepcopy(f)
      h = get_h(a, c)
      artifacts["h"] = copy.deepcopy(h)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_f())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_complex_h_perartifact].1
  ''
---
# name: test_pipeline_generation[airflow_complex_h_perartifact].2
  '
  import pathlib
  import pickle
  
  import airflow_complex_h_perart_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_a_c_for_artifact_f_and_downstream():
  
      a, c = airflow_complex_h_perart_module.get_a_c_for_artifact_f_and_downstream()
  
      if not pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").exists():
          pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").mkdir()
      pickle.dump(a, open("/tmp/airflow_complex_h_perart/variable_a.pickle", "wb"))
  
      if not pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").exists():
          pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").mkdir()
      pickle.dump(c, open("/tmp/airflow_complex_h_perart/variable_c.pickle", "wb"))
  
  
  def task_f():
  
      c = pickle.load(open("/tmp/airflow_complex_h_perart/variable_c.pickle", "rb"))
  
      f = airflow_complex_h_perart_module.get_f(c)
  
      if not pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").exists():
          pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").mkdir()
      pickle.dump(f, open("/tmp/airflow_complex_h_perart/variable_f.pickle", "wb"))
  
  
  def task_h():
  
      a = pickle.load(open("/tmp/airflow_complex_h_perart/variable_a.pickle", "rb"))
  
      c = pickle.load(open("/tmp/airflow_complex_h_perart/variable_c.pickle", "rb"))
  
      h = airflow_complex_h_perart_module.get_h(a, c)
  
      if not pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").exists():
          pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").mkdir()
      pickle.dump(h, open("/tmp/airflow_complex_h_perart/variable_h.pickle", "wb"))
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_complex_h_perart")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp").joinpath("airflow_complex_h_perart").glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_complex_h_perart_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      a_c_for_artifact_f_and_downstream = PythonOperator(
          task_id="a_c_for_artifact_f_and_downstream_task",
          python_callable=task_a_c_for_artifact_f_and_downstream,
      )
  
      f = PythonOperator(
          task_id="f_task",
          python_callable=task_f,
      )
  
      h = PythonOperator(
          task_id="h_task",
          python_callable=task_h,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      a_c_for_artifact_f_and_downstream >> f
  
      a_c_for_artifact_f_and_downstream >> h
  
      f >> teardown
  
      h >> teardown
  
      setup >> a_c_for_artifact_f_and_downstream
  
  '
---
# name: test_pipeline_generation[airflow_hidden_session_dependencies]
  '
  def get_a():
      b0 = 0
      a = b0 + 1
      return a
  
  
  def get_linear_first_for_artifact_linear_second_and_downstream():
      linear_first = 1
      return linear_first
  
  
  def get_linear_second(linear_first):
      linear_second = linear_first + 1
      return linear_second
  
  
  def get_linear_third(linear_first, linear_second):
      linear_third = linear_second + linear_first
      return linear_third
  
  
  def run_session_including_a():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a = get_a()
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_session_including_linear_second():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      linear_first = get_linear_first_for_artifact_linear_second_and_downstream()
      linear_second = get_linear_second(linear_first)
      artifacts["linear_second"] = copy.deepcopy(linear_second)
      linear_third = get_linear_third(linear_first, linear_second)
      artifacts["linear_third"] = copy.deepcopy(linear_third)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a())
      artifacts.update(run_session_including_linear_second())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_hidden_session_dependencies].1
  ''
---
# name: test_pipeline_generation[airflow_hidden_session_dependencies].2
  '
  import pathlib
  import pickle
  
  import airflow_hidden_session_dependencies_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_a():
  
      a = airflow_hidden_session_dependencies_module.get_a()
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_hidden_session_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_hidden_session_dependencies").mkdir()
      pickle.dump(
          a, open("/tmp/airflow_hidden_session_dependencies/variable_a.pickle", "wb")
      )
  
  
  def task_linear_first_for_artifact_linear_second_and_downstream():
  
      linear_first = (
          airflow_hidden_session_dependencies_module.get_linear_first_for_artifact_linear_second_and_downstream()
      )
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_hidden_session_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_hidden_session_dependencies").mkdir()
      pickle.dump(
          linear_first,
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_first.pickle",
              "wb",
          ),
      )
  
  
  def task_linear_second():
  
      linear_first = pickle.load(
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_first.pickle",
              "rb",
          )
      )
  
      linear_second = airflow_hidden_session_dependencies_module.get_linear_second(
          linear_first
      )
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_hidden_session_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_hidden_session_dependencies").mkdir()
      pickle.dump(
          linear_second,
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_second.pickle",
              "wb",
          ),
      )
  
  
  def task_linear_third():
  
      linear_first = pickle.load(
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_first.pickle",
              "rb",
          )
      )
  
      linear_second = pickle.load(
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_second.pickle",
              "rb",
          )
      )
  
      linear_third = airflow_hidden_session_dependencies_module.get_linear_third(
          linear_first, linear_second
      )
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_hidden_session_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_hidden_session_dependencies").mkdir()
      pickle.dump(
          linear_third,
          open(
              "/tmp/airflow_hidden_session_dependencies/variable_linear_third.pickle",
              "wb",
          ),
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_hidden_session_dependencies")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_hidden_session_dependencies")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_hidden_session_dependencies_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      a = PythonOperator(
          task_id="a_task",
          python_callable=task_a,
      )
  
      linear_first_for_artifact_linear_second_and_downstream = PythonOperator(
          task_id="linear_first_for_artifact_linear_second_and_downstream_task",
          python_callable=task_linear_first_for_artifact_linear_second_and_downstream,
      )
  
      linear_second = PythonOperator(
          task_id="linear_second_task",
          python_callable=task_linear_second,
      )
  
      linear_third = PythonOperator(
          task_id="linear_third_task",
          python_callable=task_linear_third,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      a >> linear_first_for_artifact_linear_second_and_downstream
  
      linear_first_for_artifact_linear_second_and_downstream >> linear_second
  
      linear_first_for_artifact_linear_second_and_downstream >> linear_third
  
      linear_second >> linear_third
  
      linear_third >> teardown
  
      setup >> a
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0].1
  ''
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_a0_b0_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_a0():
  
      a0 = airflow_pipeline_a0_b0_module.get_a0()
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0").mkdir()
      pickle.dump(a0, open("/tmp/airflow_pipeline_a0_b0/variable_a0.pickle", "wb"))
  
  
  def task_b0():
  
      b0 = airflow_pipeline_a0_b0_module.get_b0()
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0").mkdir()
      pickle.dump(b0, open("/tmp/airflow_pipeline_a0_b0/variable_b0.pickle", "wb"))
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0").glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_pipeline_a0_b0_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      a0 = PythonOperator(
          task_id="a0_task",
          python_callable=task_a0,
      )
  
      b0 = PythonOperator(
          task_id="b0_task",
          python_callable=task_b0,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      a0 >> teardown
  
      b0 >> teardown
  
      setup >> a0
  
      setup >> b0
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0_dependencies]
  '
  def get_b0():
      b0 = 0
      return b0
  
  
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_b0())
      artifacts.update(run_session_including_a0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0_dependencies].1
  ''
---
# name: test_pipeline_generation[airflow_pipeline_a0_b0_dependencies].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_a0_b0_dependencies_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_a0():
  
      a0 = airflow_pipeline_a0_b0_dependencies_module.get_a0()
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a0_b0_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0_dependencies").mkdir()
      pickle.dump(
          a0, open("/tmp/airflow_pipeline_a0_b0_dependencies/variable_a0.pickle", "wb")
      )
  
  
  def task_b0():
  
      b0 = airflow_pipeline_a0_b0_dependencies_module.get_b0()
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a0_b0_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0_dependencies").mkdir()
      pickle.dump(
          b0, open("/tmp/airflow_pipeline_a0_b0_dependencies/variable_b0.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_pipeline_a0_b0_dependencies")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a0_b0_dependencies")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_pipeline_a0_b0_dependencies_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      b0 = PythonOperator(
          task_id="b0_task",
          python_callable=task_b0,
      )
  
      a0 = PythonOperator(
          task_id="a0_task",
          python_callable=task_a0,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      a0 >> teardown
  
      b0 >> teardown
  
      setup >> a0
  
      setup >> b0
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_artifact]
  '
  import argparse
  
  
  def get_b0(b0):
  
      return b0
  
  
  def get_a(b0):
      a = b0 + 1
      return a
  
  
  def run_session_including_b0(b0=0):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0(b0)
      artifacts["b0"] = copy.deepcopy(b0)
      a = get_a(b0)
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_all_sessions(
      b0=0,
  ):
      artifacts = dict()
      artifacts.update(run_session_including_b0(b0))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--b0", type=int, default=0)
      args = parser.parse_args()
      artifacts = run_all_sessions(
          b0=args.b0,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_artifact].1
  ''
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_artifact].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_a_b0_inputpar_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_a():
  
      b0 = pickle.load(
          open("/tmp/airflow_pipeline_a_b0_inputpar/variable_b0.pickle", "rb")
      )
  
      a = airflow_pipeline_a_b0_inputpar_module.get_a(b0)
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar").mkdir()
      pickle.dump(a, open("/tmp/airflow_pipeline_a_b0_inputpar/variable_a.pickle", "wb"))
  
  
  def task_b0(b0):
  
      b0 = int(b0)
  
      b0 = airflow_pipeline_a_b0_inputpar_module.get_b0(b0)
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar").mkdir()
      pickle.dump(
          b0, open("/tmp/airflow_pipeline_a_b0_inputpar/variable_b0.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar").glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
      "params": {"b0": 0},
  }
  
  with DAG(
      dag_id="airflow_pipeline_a_b0_inputpar_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      b0 = PythonOperator(
          task_id="b0_task",
          python_callable=task_b0,
          op_kwargs={"b0": "{{ params.b0 }}"},
      )
  
      a = PythonOperator(
          task_id="a_task",
          python_callable=task_a,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      a >> teardown
  
      b0 >> a
  
      setup >> b0
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_session]
  '
  import argparse
  
  
  def get_b0(b0):
  
      return b0
  
  
  def get_a(b0):
      a = b0 + 1
      return a
  
  
  def run_session_including_b0(b0=0):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0(b0)
      artifacts["b0"] = copy.deepcopy(b0)
      a = get_a(b0)
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_all_sessions(
      b0=0,
  ):
      artifacts = dict()
      artifacts.update(run_session_including_b0(b0))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--b0", type=int, default=0)
      args = parser.parse_args()
      artifacts = run_all_sessions(
          b0=args.b0,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_session].1
  ''
---
# name: test_pipeline_generation[airflow_pipeline_a_b0_input_parameter_per_session].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_a_b0_inputpar_session_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_run_session_including_b0(b0):
  
      b0 = int(b0)
  
      artifacts = airflow_pipeline_a_b0_inputpar_session_module.run_session_including_b0(
          b0
      )
  
      b0 = artifacts["b0"]
      a = artifacts["a"]
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a_b0_inputpar_session")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar_session").mkdir()
      pickle.dump(
          b0, open("/tmp/airflow_pipeline_a_b0_inputpar_session/variable_b0.pickle", "wb")
      )
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a_b0_inputpar_session")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_a_b0_inputpar_session").mkdir()
      pickle.dump(
          a, open("/tmp/airflow_pipeline_a_b0_inputpar_session/variable_a.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath(
          "airflow_pipeline_a_b0_inputpar_session"
      )
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_a_b0_inputpar_session")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
      "params": {"b0": 0},
  }
  
  with DAG(
      dag_id="airflow_pipeline_a_b0_inputpar_session_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      run_session_including_b0 = PythonOperator(
          task_id="run_session_including_b0_task",
          python_callable=task_run_session_including_b0,
          op_kwargs={"b0": "{{ params.b0 }}"},
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      run_session_including_b0 >> teardown
  
      setup >> run_session_including_b0
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_multiple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_multiple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_multiple].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_housing_multiple_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_assets_for_artifact_y_and_downstream():
  
      assets = (
          airflow_pipeline_housing_multiple_module.get_assets_for_artifact_y_and_downstream()
      )
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").mkdir()
      pickle.dump(
          assets,
          open("/tmp/airflow_pipeline_housing_multiple/variable_assets.pickle", "wb"),
      )
  
  
  def task_p_value():
  
      assets = pickle.load(
          open("/tmp/airflow_pipeline_housing_multiple/variable_assets.pickle", "rb")
      )
  
      y = pickle.load(
          open("/tmp/airflow_pipeline_housing_multiple/variable_y.pickle", "rb")
      )
  
      p = airflow_pipeline_housing_multiple_module.get_p_value(assets, y)
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").mkdir()
      pickle.dump(
          p, open("/tmp/airflow_pipeline_housing_multiple/variable_p.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_multiple")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  def task_y():
  
      assets = pickle.load(
          open("/tmp/airflow_pipeline_housing_multiple/variable_assets.pickle", "rb")
      )
  
      y = airflow_pipeline_housing_multiple_module.get_y(assets)
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_multiple").mkdir()
      pickle.dump(
          y, open("/tmp/airflow_pipeline_housing_multiple/variable_y.pickle", "wb")
      )
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_pipeline_housing_multiple_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      assets_for_artifact_y_and_downstream = PythonOperator(
          task_id="assets_for_artifact_y_and_downstream_task",
          python_callable=task_assets_for_artifact_y_and_downstream,
      )
  
      y = PythonOperator(
          task_id="y_task",
          python_callable=task_y,
      )
  
      p_value = PythonOperator(
          task_id="p_value_task",
          python_callable=task_p_value,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      assets_for_artifact_y_and_downstream >> p_value
  
      assets_for_artifact_y_and_downstream >> y
  
      p_value >> teardown
  
      setup >> assets_for_artifact_y_and_downstream
  
      y >> p_value
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_simple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_p_value():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      clf = RandomForestClassifier(random_state=0)
      y = assets["is_new"]
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_p_value():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      p = get_p_value()
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_p_value())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_simple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_simple].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_housing_simple_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_p_value():
  
      p = airflow_pipeline_housing_simple_module.get_p_value()
  
      if not pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_simple").exists():
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_simple").mkdir()
      pickle.dump(p, open("/tmp/airflow_pipeline_housing_simple/variable_p.pickle", "wb"))
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_simple")
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_simple")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_pipeline_housing_simple_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      p_value = PythonOperator(
          task_id="p_value_task",
          python_callable=task_p_value,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      p_value >> teardown
  
      setup >> p_value
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_w_dependencies]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_w_dependencies].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_generation[airflow_pipeline_housing_w_dependencies].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_housing_w_dependencies_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_assets_for_artifact_y_and_downstream():
  
      assets = (
          airflow_pipeline_housing_w_dependencies_module.get_assets_for_artifact_y_and_downstream()
      )
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_w_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_w_dependencies").mkdir()
      pickle.dump(
          assets,
          open(
              "/tmp/airflow_pipeline_housing_w_dependencies/variable_assets.pickle", "wb"
          ),
      )
  
  
  def task_p_value():
  
      assets = pickle.load(
          open(
              "/tmp/airflow_pipeline_housing_w_dependencies/variable_assets.pickle", "rb"
          )
      )
  
      y = pickle.load(
          open("/tmp/airflow_pipeline_housing_w_dependencies/variable_y.pickle", "rb")
      )
  
      p = airflow_pipeline_housing_w_dependencies_module.get_p_value(assets, y)
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_w_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_w_dependencies").mkdir()
      pickle.dump(
          p, open("/tmp/airflow_pipeline_housing_w_dependencies/variable_p.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath(
          "airflow_pipeline_housing_w_dependencies"
      )
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_w_dependencies")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  def task_y():
  
      assets = pickle.load(
          open(
              "/tmp/airflow_pipeline_housing_w_dependencies/variable_assets.pickle", "rb"
          )
      )
  
      y = airflow_pipeline_housing_w_dependencies_module.get_y(assets)
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_housing_w_dependencies")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_housing_w_dependencies").mkdir()
      pickle.dump(
          y, open("/tmp/airflow_pipeline_housing_w_dependencies/variable_y.pickle", "wb")
      )
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
  }
  
  with DAG(
      dag_id="airflow_pipeline_housing_w_dependencies_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      assets_for_artifact_y_and_downstream = PythonOperator(
          task_id="assets_for_artifact_y_and_downstream_task",
          python_callable=task_assets_for_artifact_y_and_downstream,
      )
  
      y = PythonOperator(
          task_id="y_task",
          python_callable=task_y,
      )
  
      p_value = PythonOperator(
          task_id="p_value_task",
          python_callable=task_p_value,
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      assets_for_artifact_y_and_downstream >> p_value
  
      assets_for_artifact_y_and_downstream >> y
  
      p_value >> teardown
  
      setup >> assets_for_artifact_y_and_downstream
  
      y >> p_value
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_two_input_parameter]
  '
  import argparse
  
  
  def get_pn(n, p):
      pn = p * n
      return pn
  
  
  def run_session_including_pn(
      p="p",
      n=5,
  ):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      pn = get_pn(n, p)
      artifacts["pn"] = copy.deepcopy(pn)
      return artifacts
  
  
  def run_all_sessions(
      n=5,
      p="p",
  ):
      artifacts = dict()
      artifacts.update(run_session_including_pn(p, n))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--n", type=int, default=5)
      parser.add_argument("--p", type=str, default="p")
      args = parser.parse_args()
      artifacts = run_all_sessions(
          n=args.n,
          p=args.p,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[airflow_pipeline_two_input_parameter].1
  ''
---
# name: test_pipeline_generation[airflow_pipeline_two_input_parameter].2
  '
  import pathlib
  import pickle
  
  import airflow_pipeline_two_input_parameter_module
  from airflow import DAG
  from airflow.operators.python_operator import PythonOperator
  from airflow.utils.dates import days_ago
  
  
  def task_pn(n, p):
  
      n = int(n)
  
      p = str(p)
  
      pn = airflow_pipeline_two_input_parameter_module.get_pn(n, p)
  
      if (
          not pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_two_input_parameter")
          .exists()
      ):
          pathlib.Path("/tmp").joinpath("airflow_pipeline_two_input_parameter").mkdir()
      pickle.dump(
          pn, open("/tmp/airflow_pipeline_two_input_parameter/variable_pn.pickle", "wb")
      )
  
  
  def task_setup():
  
      pickle_folder = pathlib.Path("/tmp").joinpath(
          "airflow_pipeline_two_input_parameter"
      )
      if not pickle_folder.exists():
          pickle_folder.mkdir()
  
  
  def task_teardown():
  
      pickle_files = (
          pathlib.Path("/tmp")
          .joinpath("airflow_pipeline_two_input_parameter")
          .glob("*.pickle")
      )
      for f in pickle_files:
          f.unlink()
  
  
  default_dag_args = {
      "owner": "airflow",
      "retries": 2,
      "start_date": days_ago(1),
      "params": {"p": "p", "n": 5},
  }
  
  with DAG(
      dag_id="airflow_pipeline_two_input_parameter_dag",
      schedule_interval="*/15 * * * *",
      max_active_runs=1,
      catchup=False,
      default_args=default_dag_args,
  ) as dag:
  
      pn = PythonOperator(
          task_id="pn_task",
          python_callable=task_pn,
          op_kwargs={"n": "{{ params.n }}", "p": "{{ params.p }}"},
      )
  
      setup = PythonOperator(
          task_id="setup_task",
          python_callable=task_setup,
      )
  
      teardown = PythonOperator(
          task_id="teardown_task",
          python_callable=task_teardown,
      )
  
      pn >> teardown
  
      setup >> pn
  
  '
---
# name: test_pipeline_generation[argo_pipeline_a0_b0_step_per_session]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[argo_pipeline_a0_b0_step_per_session].1
  ''
---
# name: test_pipeline_generation[argo_pipeline_a0_b0_step_per_session].2
  '
  import base64
  import errno
  import os
  from typing import Optional
  
  from hera import Artifact, ImagePullPolicy, set_global_task_image
  from hera.task import Task
  from hera.workflow import Workflow
  from hera.workflow_service import WorkflowService
  from kubernetes import client, config
  
  
  def get_sa_token(
      service_account: str,
      namespace: str = "argo",
      config_file: Optional[str] = None,
  ):
      """
      Configues the kubernetes client and returns the service account token for the
      specified service account in the specified namespace.
      This is used in the case the local kubeconfig exists.
      """
      if config_file is not None and not os.path.isfile(config_file):
          raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), config_file)
  
      config.load_kube_config(config_file=config_file)
      v1 = client.CoreV1Api()
      if v1.read_namespaced_service_account(service_account, namespace).secrets is None:
          print("No secrets found in namespace: %s" % namespace)
          return "None"
  
      secret_name = (
          v1.read_namespaced_service_account(service_account, namespace).secrets[0].name
      )
  
      sec = v1.read_namespaced_secret(secret_name, namespace).data
      return base64.b64decode(sec["token"]).decode()
  
  
  def task_run_session_including_a0():
      import pathlib
      import pickle
  
      import argo_pipeline_a0_b0_module
  
      artifacts = argo_pipeline_a0_b0_module.run_session_including_a0()
  
      a0 = artifacts["a0"]
  
      if not pathlib.Path("/tmp").joinpath("argo_pipeline_a0_b0").exists():
          pathlib.Path("/tmp").joinpath("argo_pipeline_a0_b0").mkdir()
      pickle.dump(a0, open("/tmp/argo_pipeline_a0_b0/variable_a0.pickle", "wb"))
  
  
  def task_run_session_including_b0():
      import pathlib
      import pickle
  
      import argo_pipeline_a0_b0_module
  
      artifacts = argo_pipeline_a0_b0_module.run_session_including_b0()
  
      b0 = artifacts["b0"]
  
      if not pathlib.Path("/tmp").joinpath("argo_pipeline_a0_b0").exists():
          pathlib.Path("/tmp").joinpath("argo_pipeline_a0_b0").mkdir()
      pickle.dump(b0, open("/tmp/argo_pipeline_a0_b0/variable_b0.pickle", "wb"))
  
  
  def task_setup():
      import pathlib
      import pickle
  
      import argo_pipeline_a0_b0_module
  
      pass
  
  
  def task_teardown():
      import pathlib
      import pickle
  
      import argo_pipeline_a0_b0_module
  
      pass
  
  
  ws = WorkflowService(
      host="https://localhost:2746",
      verify_ssl=False,
      token=get_sa_token("argo", "argo", os.path.expanduser("~/.kube/config")),
      namespace="argo",
  )
  
  with Workflow("argo-pipeline-a0-b0", service=ws) as w:
  
      set_global_task_image("argo_pipeline_a0_b0:lineapy")
  
      run_session_including_a0 = Task(
          "run-session-including-a0",
          task_run_session_including_a0,
          image_pull_policy=ImagePullPolicy.Never,
          outputs=[
              Artifact(
                  "a0",
                  "/tmp/argo_pipeline_a0_b0/variable_a0.pickle",
              ),
          ],
      )
  
      run_session_including_b0 = Task(
          "run-session-including-b0",
          task_run_session_including_b0,
          image_pull_policy=ImagePullPolicy.Never,
          outputs=[
              Artifact(
                  "b0",
                  "/tmp/argo_pipeline_a0_b0/variable_b0.pickle",
              ),
          ],
      )
  
      setup = Task(
          "setup",
          task_setup,
          image_pull_policy=ImagePullPolicy.Never,
      )
  
      teardown = Task(
          "teardown",
          task_teardown,
          image_pull_policy=ImagePullPolicy.Never,
      )
  
      setup >> run_session_including_b0
  
      setup >> run_session_including_a0
  
      run_session_including_b0 >> teardown
  
      run_session_including_a0 >> teardown
  
  
  w.create()
  
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact]
  '
  def get_b0():
      b0 = 0
      return b0
  
  
  def get_a(b0):
      a = b0 + 1
      return a
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      a = get_a(b0)
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact].1
  ''
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact].2
  '
  stages:
  
      b0:
          cmd: python task_b0.py
          deps:
              - dvc_pipeline_a_b0_stageperartifact_module.py
              - task_b0.py
          outs:
              - b0.pickle
  
      a:
          cmd: python task_a.py
          deps:
              - dvc_pipeline_a_b0_stageperartifact_module.py
              - task_a.py
              - b0.pickle
          outs:
              - a.pickle
  
  
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact].3
  '
  
  import dvc_pipeline_a_b0_stageperartifact_module
  import pickle
  
  def task_a():
      b0 = pickle.load(open('b0.pickle','rb'))
      a = dvc_pipeline_a_b0_stageperartifact_module.get_a(b0)
      pickle.dump(a, open('a.pickle','wb'))
  
  if __name__ == "__main__":
      task_a()
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact].4
  '
  
  import dvc_pipeline_a_b0_stageperartifact_module
  import pickle
  
  def task_b0():
      b0 = dvc_pipeline_a_b0_stageperartifact_module.get_b0()
      pickle.dump(b0, open('b0.pickle','wb'))
  
  if __name__ == "__main__":
      task_b0()
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact].5
  ''
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact_with_input_parameter]
  '
  import argparse
  
  
  def get_a(b0):
      a = b0 + 1
      return a
  
  
  def run_session_including_a(b0=0):
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a = get_a(b0)
      artifacts["a"] = copy.deepcopy(a)
      return artifacts
  
  
  def run_all_sessions(
      b0=0,
  ):
      artifacts = dict()
      artifacts.update(run_session_including_a(b0))
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      parser = argparse.ArgumentParser()
      parser.add_argument("--b0", type=int, default=0)
      args = parser.parse_args()
      artifacts = run_all_sessions(
          b0=args.b0,
      )
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact_with_input_parameter].1
  ''
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact_with_input_parameter].2
  '
  stages:
  
      a:
          cmd: python task_a.py
          deps:
              - dvc_pipeline_a_b0_stageperartifact_module.py
              - task_a.py
          outs:
              - a.pickle
  
  
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact_with_input_parameter].3
  '
  import dvc.api
  
  import dvc_pipeline_a_b0_stageperartifact_module
  import pickle
  
  def task_a(b0):
      a = dvc_pipeline_a_b0_stageperartifact_module.get_a(b0)
      pickle.dump(a, open('a.pickle','wb'))
  
  if __name__ == "__main__":
      b0 = dvc.api.params_show()["b0"]
      task_a(b0)
  '
---
# name: test_pipeline_generation[dvc_pipeline_a_b0_stage_per_artifact_with_input_parameter].4
  '
  b0: 0
    
  '
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_artifact]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_artifact].1
  ''
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_artifact].2
  '
  import kfp
  from kfp.components import create_component_from_func
  
  
  def task_a0(variable_a0_path: kfp.components.OutputPath(str)):
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_artifact_module
  
      a0 = kubeflow_pipeline_a0_b0_component_artifact_module.get_a0()
  
      pickle.dump(a0, open(variable_a0_path, "wb"))
  
  
  def task_b0(variable_b0_path: kfp.components.OutputPath(str)):
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_artifact_module
  
      b0 = kubeflow_pipeline_a0_b0_component_artifact_module.get_b0()
  
      pickle.dump(b0, open(variable_b0_path, "wb"))
  
  
  def task_setup():
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_artifact_module
  
      pass
  
  
  def task_teardown():
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_artifact_module
  
      pass
  
  
  a0_component = create_component_from_func(
      task_a0, base_image="kubeflow_pipeline_a0_b0_component_artifact:lineapy"
  )
  
  b0_component = create_component_from_func(
      task_b0, base_image="kubeflow_pipeline_a0_b0_component_artifact:lineapy"
  )
  
  setup_component = create_component_from_func(
      task_setup, base_image="kubeflow_pipeline_a0_b0_component_artifact:lineapy"
  )
  
  teardown_component = create_component_from_func(
      task_teardown, base_image="kubeflow_pipeline_a0_b0_component_artifact:lineapy"
  )
  
  
  client = kfp.Client(host="http://localhost:3000")
  
  
  @kfp.dsl.pipeline(
      name="kubeflow_pipeline_a0_b0_component_artifact_dag",
  )
  def kubeflow_pipeline_a0_b0_component_artifact():
  
      task_a0 = a0_component()
      task_b0 = b0_component()
      task_setup = setup_component()
      task_teardown = teardown_component()
  
      task_a0.after(task_setup)
  
      task_b0.after(task_setup)
  
      task_teardown.after(task_a0)
  
      task_teardown.after(task_b0)
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  # Create a pipeline run, using the client you initialized in a prior step.
  client.create_run_from_pipeline_func(
      kubeflow_pipeline_a0_b0_component_artifact, arguments=pipeline_arguments
  )
  
  '
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_session]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_session].1
  ''
---
# name: test_pipeline_generation[kubeflow_pipeline_a0_b0_component_session].2
  '
  import kfp
  from kfp.components import create_component_from_func
  
  
  def task_run_session_including_a0(variable_a0_path: kfp.components.OutputPath(str)):
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_session_module
  
      artifacts = (
          kubeflow_pipeline_a0_b0_component_session_module.run_session_including_a0()
      )
  
      a0 = artifacts["a0"]
  
      pickle.dump(a0, open(variable_a0_path, "wb"))
  
  
  def task_run_session_including_b0(variable_b0_path: kfp.components.OutputPath(str)):
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_session_module
  
      artifacts = (
          kubeflow_pipeline_a0_b0_component_session_module.run_session_including_b0()
      )
  
      b0 = artifacts["b0"]
  
      pickle.dump(b0, open(variable_b0_path, "wb"))
  
  
  def task_setup():
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_session_module
  
      pass
  
  
  def task_teardown():
      import pathlib
      import pickle
  
      import kubeflow_pipeline_a0_b0_component_session_module
  
      pass
  
  
  run_session_including_a0_component = create_component_from_func(
      task_run_session_including_a0,
      base_image="kubeflow_pipeline_a0_b0_component_session:lineapy",
  )
  
  run_session_including_b0_component = create_component_from_func(
      task_run_session_including_b0,
      base_image="kubeflow_pipeline_a0_b0_component_session:lineapy",
  )
  
  setup_component = create_component_from_func(
      task_setup, base_image="kubeflow_pipeline_a0_b0_component_session:lineapy"
  )
  
  teardown_component = create_component_from_func(
      task_teardown, base_image="kubeflow_pipeline_a0_b0_component_session:lineapy"
  )
  
  
  client = kfp.Client(host="http://localhost:3000")
  
  
  @kfp.dsl.pipeline(
      name="kubeflow_pipeline_a0_b0_component_session_dag",
  )
  def kubeflow_pipeline_a0_b0_component_session():
  
      task_run_session_including_a0 = run_session_including_a0_component()
      task_run_session_including_b0 = run_session_including_b0_component()
      task_setup = setup_component()
      task_teardown = teardown_component()
  
      task_run_session_including_a0.after(task_setup)
  
      task_run_session_including_b0.after(task_setup)
  
      task_teardown.after(task_run_session_including_a0)
  
      task_teardown.after(task_run_session_including_b0)
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  # Create a pipeline run, using the client you initialized in a prior step.
  client.create_run_from_pipeline_func(
      kubeflow_pipeline_a0_b0_component_session, arguments=pipeline_arguments
  )
  
  '
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_artifact]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_artifact].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_artifact].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_a0_b0_task_artifact_module
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote(num_returns=1)
  def task_a0():
  
      a0 = ray_pipeline_a0_b0_task_artifact_module.get_a0()
  
      return a0
  
  
  @ray.remote(num_returns=1)
  def task_b0():
  
      b0 = ray_pipeline_a0_b0_task_artifact_module.get_b0()
  
      return b0
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  a0 = task_a0.remote()
  b0 = task_b0.remote()
  
  # Execute actors to get remote objects
  # Make changes here to access any additional objects needed.
  ray.get([a0])
  ray.get([b0])
  
  '
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_session]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_session].1
  'packaging==21.3'
---
# name: test_pipeline_generation[ray_pipeline_a0_b0_task_session].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_a0_b0_task_session_module
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote(num_returns=1)
  def task_run_session_including_a0():
  
      artifacts = ray_pipeline_a0_b0_task_session_module.run_session_including_a0()
  
      a0 = artifacts["a0"]
  
      return a0
  
  
  @ray.remote(num_returns=1)
  def task_run_session_including_b0():
  
      artifacts = ray_pipeline_a0_b0_task_session_module.run_session_including_b0()
  
      b0 = artifacts["b0"]
  
      return b0
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  a0 = task_run_session_including_a0.remote()
  b0 = task_run_session_including_b0.remote()
  
  # Execute actors to get remote objects
  # Make changes here to access any additional objects needed.
  ray.get([a0])
  ray.get([b0])
  
  '
---
# name: test_pipeline_generation[ray_pipeline_housing_w_dependencies]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[ray_pipeline_housing_w_dependencies].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  packaging==21.3
  '
---
# name: test_pipeline_generation[ray_pipeline_housing_w_dependencies].2
  '
  import pathlib
  import pickle
  
  import ray
  import ray_pipeline_housing_w_dependencies_module
  from packaging import version
  
  ray.init(runtime_env={"working_dir": "."}, storage="/tmp")
  
  
  @ray.remote
  def task_assets_for_artifact_y_and_downstream():
  
      assets = (
          ray_pipeline_housing_w_dependencies_module.get_assets_for_artifact_y_and_downstream()
      )
  
      return assets
  
  
  @ray.remote
  def task_y(assets):
  
      y = ray_pipeline_housing_w_dependencies_module.get_y(assets)
  
      return y
  
  
  @ray.remote
  def task_p_value(assets, y):
  
      p = ray_pipeline_housing_w_dependencies_module.get_p_value(assets, y)
  
      return p
  
  
  # Specify argument values for your pipeline run.
  pipeline_arguments = {}
  
  assets = task_assets_for_artifact_y_and_downstream.bind()
  y = task_y.bind(assets)
  p = task_p_value.bind(assets, y)
  
  if version.parse(ray.__version__) < version.parse("2.0"):
      raise RuntimeError(
          f"Ray Workflows requires version >2.0 but {ray.__version__} was found"
      )
  ray.workflow.run(p)
  
  '
---
# name: test_pipeline_generation[script_pipeline_a0_b0]
  '
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def get_b0():
      b0 = 0
      return b0
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_a0())
      artifacts.update(run_session_including_b0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[script_pipeline_a0_b0].1
  ''
---
# name: test_pipeline_generation[script_pipeline_a0_b0_dependencies]
  '
  def get_b0():
      b0 = 0
      return b0
  
  
  def get_a0():
      a0 = 0
      a0 += 1
      return a0
  
  
  def run_session_including_b0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      b0 = get_b0()
      artifacts["b0"] = copy.deepcopy(b0)
      return artifacts
  
  
  def run_session_including_a0():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      a0 = get_a0()
      artifacts["a0"] = copy.deepcopy(a0)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_b0())
      artifacts.update(run_session_including_a0())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[script_pipeline_a0_b0_dependencies].1
  ''
---
# name: test_pipeline_generation[script_pipeline_housing_multiple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[script_pipeline_housing_multiple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_generation[script_pipeline_housing_simple]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_p_value():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      clf = RandomForestClassifier(random_state=0)
      y = assets["is_new"]
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_p_value():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      p = get_p_value()
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_p_value())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[script_pipeline_housing_simple].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_generation[script_pipeline_housing_w_dependencies]
  '
  import pandas as pd
  from sklearn.ensemble import RandomForestClassifier
  
  
  def get_assets_for_artifact_y_and_downstream():
      assets = pd.read_csv(
          "https://raw.githubusercontent.com/LineaLabs/lineapy/main/tests/ames_train_cleaned.csv"
      )
  
      def is_new(col):
          return col > 1970
  
      assets["is_new"] = is_new(assets["Year_Built"])
      return assets
  
  
  def get_y(assets):
      y = assets["is_new"]
      return y
  
  
  def get_p_value(assets, y):
      clf = RandomForestClassifier(random_state=0)
      x = assets[["SalePrice", "Lot_Area", "Garage_Area"]]
      clf.fit(x, y)
      p = clf.predict([[100 * 1000, 10, 4]])
      return p
  
  
  def run_session_including_y():
      # Given multiple artifacts, we need to save each right after
      # its calculation to protect from any irrelevant downstream
      # mutations (e.g., inside other artifact calculations)
      import copy
  
      artifacts = dict()
      assets = get_assets_for_artifact_y_and_downstream()
      y = get_y(assets)
      artifacts["y"] = copy.deepcopy(y)
      p = get_p_value(assets, y)
      artifacts["p value"] = copy.deepcopy(p)
      return artifacts
  
  
  def run_all_sessions():
      artifacts = dict()
      artifacts.update(run_session_including_y())
      return artifacts
  
  
  if __name__ == "__main__":
      # Edit this section to customize the behavior of artifacts
      artifacts = run_all_sessions()
      print(artifacts)
  
  '
---
# name: test_pipeline_generation[script_pipeline_housing_w_dependencies].1
  '
  pandas==1.3.5
  scikit-learn==1.0.2
  '
---
# name: test_pipeline_test_generation[script_complex_h_perart]
  '
  import os
  import pickle
  import unittest
  import warnings
  from pathlib import Path
  from typing import Callable
  
  from script_complex_h_perart_module import (
      get_a_c_for_artifact_f_and_downstream,
      get_f,
      get_h,
  )
  
  
  def safe_load_pickle(
      path_to_file: Path,
      alt_val_func: Callable = lambda: FileNotFoundError,
      save_alt_val: bool = False,
  ):
      """
      Load the specified pickle file if it exists.
      If not, use the provided function to generate and return
      an alternative value (the desired execution should be wrapped
      inside a lambda function to delay actual execution until needed).
      """
      if os.path.exists(path_to_file):
          with open(path_to_file, "rb") as fp:
              file_value = pickle.load(fp)
          return file_value
      else:
          alt_value = alt_val_func()
          if save_alt_val is True:
              # Store value to avoid recompute across test cases
              with open(path_to_file, "wb") as fp:
                  pickle.dump(alt_value, fp)
          return alt_value
  
  
  class TestScriptComplexHPerart(unittest.TestCase):
      art_pkl_dir: Path
  
      def setUp(self) -> None:
          # Add any processes to execute before each test in this class
          pass
  
      def tearDown(self) -> None:
          # Add any processes to execute after each test in this class
          pass
  
      @classmethod
      def setUpClass(cls) -> None:
          # Specify location where sample output files are stored for comparison
          cls.art_pkl_dir = Path(__file__).parent / "sample_output"
  
          # Add any processes to execute once before all tests in this class run
          pass
  
      @classmethod
      def tearDownClass(cls) -> None:
          # Delete pickle files for intermediate (non-artifact) values
          for intermediate_output_name in ["a_c_for_artifact_f_and_downstream"]:
              path_to_file = cls.art_pkl_dir / f"{intermediate_output_name}.pkl"
              if os.path.exists(path_to_file):
                  os.remove(path_to_file)
  
          # Add any processes to execute once after all tests in this class run
          pass
  
      def test_get_a_c_for_artifact_f_and_downstream(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          pass
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_a_c_for_artifact_f_and_downstream()
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "a_c_for_artifact_f_and_downstream.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
      def test_get_f(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          a, c = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "a_c_for_artifact_f_and_downstream.pkl"),
              alt_val_func=lambda: get_a_c_for_artifact_f_and_downstream(),
              save_alt_val=True,
          )
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_f(c)
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "f.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
      def test_get_h(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          a, c = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "a_c_for_artifact_f_and_downstream.pkl"),
              alt_val_func=lambda: get_a_c_for_artifact_f_and_downstream(),
              save_alt_val=True,
          )
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_h(a, c)
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "h.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
  '
---
# name: test_pipeline_test_generation[script_pipeline_a0_b0_dependencies]
  '
  import os
  import pickle
  import unittest
  import warnings
  from pathlib import Path
  from typing import Callable
  
  from script_pipeline_a0_b0_dependencies_module import get_a0, get_b0
  
  
  def safe_load_pickle(
      path_to_file: Path,
      alt_val_func: Callable = lambda: FileNotFoundError,
      save_alt_val: bool = False,
  ):
      """
      Load the specified pickle file if it exists.
      If not, use the provided function to generate and return
      an alternative value (the desired execution should be wrapped
      inside a lambda function to delay actual execution until needed).
      """
      if os.path.exists(path_to_file):
          with open(path_to_file, "rb") as fp:
              file_value = pickle.load(fp)
          return file_value
      else:
          alt_value = alt_val_func()
          if save_alt_val is True:
              # Store value to avoid recompute across test cases
              with open(path_to_file, "wb") as fp:
                  pickle.dump(alt_value, fp)
          return alt_value
  
  
  class TestScriptPipelineA0B0Dependencies(unittest.TestCase):
      art_pkl_dir: Path
  
      def setUp(self) -> None:
          # Add any processes to execute before each test in this class
          pass
  
      def tearDown(self) -> None:
          # Add any processes to execute after each test in this class
          pass
  
      @classmethod
      def setUpClass(cls) -> None:
          # Specify location where sample output files are stored for comparison
          cls.art_pkl_dir = Path(__file__).parent / "sample_output"
  
          # Add any processes to execute once before all tests in this class run
          pass
  
      @classmethod
      def tearDownClass(cls) -> None:
          # Add any processes to execute once after all tests in this class run
          pass
  
      def test_get_b0(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          pass
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_b0()
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "b0.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
      def test_get_a0(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          pass
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_a0()
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "a0.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
  '
---
# name: test_pipeline_test_generation[script_pipeline_housing_w_dependencies]
  '
  import os
  import pickle
  import unittest
  import warnings
  from pathlib import Path
  from typing import Callable
  
  from script_pipeline_housing_w_dependencies_module import (
      get_assets_for_artifact_y_and_downstream,
      get_p_value,
      get_y,
  )
  
  
  def safe_load_pickle(
      path_to_file: Path,
      alt_val_func: Callable = lambda: FileNotFoundError,
      save_alt_val: bool = False,
  ):
      """
      Load the specified pickle file if it exists.
      If not, use the provided function to generate and return
      an alternative value (the desired execution should be wrapped
      inside a lambda function to delay actual execution until needed).
      """
      if os.path.exists(path_to_file):
          with open(path_to_file, "rb") as fp:
              file_value = pickle.load(fp)
          return file_value
      else:
          alt_value = alt_val_func()
          if save_alt_val is True:
              # Store value to avoid recompute across test cases
              with open(path_to_file, "wb") as fp:
                  pickle.dump(alt_value, fp)
          return alt_value
  
  
  class TestScriptPipelineHousingWDependencies(unittest.TestCase):
      art_pkl_dir: Path
  
      def setUp(self) -> None:
          # Add any processes to execute before each test in this class
          pass
  
      def tearDown(self) -> None:
          # Add any processes to execute after each test in this class
          pass
  
      @classmethod
      def setUpClass(cls) -> None:
          # Specify location where sample output files are stored for comparison
          cls.art_pkl_dir = Path(__file__).parent / "sample_output"
  
          # Add any processes to execute once before all tests in this class run
          pass
  
      @classmethod
      def tearDownClass(cls) -> None:
          # Delete pickle files for intermediate (non-artifact) values
          for intermediate_output_name in ["assets_for_artifact_y_and_downstream"]:
              path_to_file = cls.art_pkl_dir / f"{intermediate_output_name}.pkl"
              if os.path.exists(path_to_file):
                  os.remove(path_to_file)
  
          # Add any processes to execute once after all tests in this class run
          pass
  
      def test_get_assets_for_artifact_y_and_downstream(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          pass
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_assets_for_artifact_y_and_downstream()
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(
                  self.art_pkl_dir / "assets_for_artifact_y_and_downstream.pkl"
              ),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
      def test_get_y(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          assets = safe_load_pickle(
              path_to_file=(
                  self.art_pkl_dir / "assets_for_artifact_y_and_downstream.pkl"
              ),
              alt_val_func=lambda: get_assets_for_artifact_y_and_downstream(),
              save_alt_val=True,
          )
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_y(assets)
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "y.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
      def test_get_p_value(self) -> None:
          """
          NOTE: The code below is provided as scaffold/template.
          Please adapt it to your specific testing context.
          [TODO: ADD LINK TO WEB DOCUMENTATION].
          """
          # Prepare function input (adapt as needed)
          assets = safe_load_pickle(
              path_to_file=(
                  self.art_pkl_dir / "assets_for_artifact_y_and_downstream.pkl"
              ),
              alt_val_func=lambda: get_assets_for_artifact_y_and_downstream(),
              save_alt_val=True,
          )
          y = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "y.pkl"),
              alt_val_func=lambda: get_y(assets),
              save_alt_val=True,
          )
  
          # Generate function output (adapt as needed)
          sample_output_generated = get_p_value(assets, y)
  
          # Perform tests (add/adapt as needed)
          sample_output_expected = safe_load_pickle(
              path_to_file=(self.art_pkl_dir / "p_value.pkl"),
              alt_val_func=lambda: FileNotFoundError,
          )
          try:
              self.assertEqual(sample_output_generated, sample_output_expected)
          except Exception:
              warnings.warn(
                  "Test failed, but this may be due to our limited templating. "
                  "Please adapt the test as needed."
              )
  
  '
---
